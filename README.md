# Выполнение тестового задания Кросс-Информ на позицию младший разработчик C#
## Задание
Необходимо написать консольное приложение на C#, выполняющее частотный анализ текста.
Входные параметры: путь к текстовому файлу.
Выходные результаты: вывести на экран 10 самых часто встречающихся в тексте триплетов (3 идущих подряд буквы слова) и число их повторений, и на последней строке время работы программы в миллисекундах.
Требования: программа должна обрабатывать текст в многопоточном режиме.
Оцениваться будет правильность решения, качество кода и быстродействие программы.
> Внимание в корне репозитория находится архив с готовой программой. Крайне опасно запускать неизвестные исполняемые программы в неизолированной программной среде. Я разместил его для соответствия формальным требованиям задания (написать консольное приложение, выполняющее...). Лучшим решением будет прочтение и анализ кода вручную и, при необходимости, сборка программы из клонированного репозитория из Visual Studio или с помощью .NET CLI(dotnet publish).
## Особенности реализации
1. Цеоевая платформа .NET 6.
2. Одной из проблем, с которой можно столкнуться при решении данной задачи, это нехватка памяти. Если файл будет слишком большим, то, возможно, не удастся загрузить его в оперативную память целиком. Поэтому принято решение читать файл построчно. Это не исключает,но снижает риск OutOfMemory. С другой стороны, если загрузить в память сразу все строки, то возможно обрабатывать их параллельно. С другой стороны, на мой взгляд, не стоит хранить в текстовых файлах гигабайты информации.
2. Функциональность StreamReader предполагает возможность многопоточного доступа к файлу "на чтение". Но физически диски могут могут читать только одну область(страницу, сектор) своей памяти в один момент времени.
3. Каждая строка делится на подстроки по символами '.', ',', '!', '?', ';', ':', '—'. Чаще всего этими символами разделяются предложения. Можно задействовать гораздо больше разделителей, но это серьёзно скажется на производительности метода Split. Кроме того, накладные расходы от многопоточной обработки большого множества мелких строк могут превзойти выигрыш от многопоточности. Поэтому делю текст на предложения. И уже предложения обрабатываю многопоточно.
4. При открытии файла обработаны ошибки: "не задан путь к файлу" ,"файл не найден", "слишком длинный путь", "не найдена папка", "нет прав доступа к файлу", "неверный формат", "ошибки подсистемы ввода/вывода". Все прочие возможные ошибки обрабатываются в общем блоке "Exception"
5. Задача поиска триплетов является разновидностью задач поиска N-gram и относится к задачам "Natural Language Processing"
6. Функциии по поиску N-gram находятся в классе Ngram.
Функция GetMostFreqNgramsAsync позволяет найти наиболее часто встречающиеся Ngram произвольного размера.
GetMostFreqBigramsAsync позволяет найти наиболее часто встречающиеся bigrams.
GetMostFreqTrigramsAsync позволяет найти наиболее часто встречающиеся trigrams(триплеты).
7. Алгоритм поиска. Предложения анализируются параллельно с помощью PLINQ. Каждое предложение анализируется посимвольно. Char.IsLetter позволяет избавиться от знаков пунктуации, эмоджи и прочих небуквенных символов с учётом культуры. Далее методом "скользящее окно" из ngram составляется потокобезопасный словарь, где ключ - ngram, значение - количество. Максимальное количество потоков установлено в ThreadPool.GetMaxThreads, так как при больших файлах, производительность начала снижаться из-за переключений контекстов. Я заметил это на файлах 350к строк, на процессоре Amd A6.
8. С помощью методов расширения LINQ To Objects словарь "маппится" в коллекцию структур, сортируется и выбирается необходимое количетсво топовых ngram.
9. Время работы программы оценивается с помощью экземляров класса Stopwatch.
## Дополнительно
1. Помимо времени выполнения программы, отдельно оценивается время выполнения метода по поиску trigram.
2. Написаны тесты core логики. Сознательно покрыты не все случаи.
## Trade off
1. Весь текст не приводится к единому регистру. В требованиях это не обозначено, но для текстового анализа регистр может быть важен.
2. Остаётся открытой проблема OutOfMemory. Это критическая для работы ошибка и она не обрабатывается. Если данная функциональность будет использована в качестве сервиса в контейнере, то проблему можно решить с помощью restart-policy.
3. OutOfMemory может также возникнуть, если весь текст в файле представляет собой одну строку. Также при подходе, реализованном в моём классе не задействуется параллельность вычислений.
4. Оценка по требуемой памяти для словаря trigram. Структура NgramNode - 10 байт. В худшем случае при наличии в тексте всех trigram размер коллекции будет 50116*10 байт для русского языка (Сочетание с повторениями из (33мал+33бол) по 3). Около 500Кб для полного словаря. К этому добавляется необходимая память на куче для промежуточных короткоживущих(generation 0) ссылочных объектов. Негативного влияния  Garbage Collector не замечено.
5. Можно определить размер файла, не открывая его(FileInfo), затем принять решение считывать ео построчно или загрузить в память и обработать целиком. Я не реализовал это, так как не нашёл 100% верного, платформонезависимого способа определить количество реально доступной на машине памяти. А размер виртуальной выделенной памяти можем изменяться под контролем ОС.
6. В качестве спорного способа увеличения производительности в ущерб расходу памяти могу предложить создавать пулы для промежуточных ссылочных переменных, тем самым уменьшить количетсво аллокаций памяти в куче.
7. Если это микросервис я бы сделал его на python с примененем готового решения  из пакета [nltk.util](https://www.nltk.org/_modules/nltk/util.html)